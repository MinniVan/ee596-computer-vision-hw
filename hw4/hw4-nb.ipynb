{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ae47f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def CIFAR10_dataset_a():\n",
    "    \"\"\"write the code to grab a single mini-batch of 4 images from the training set, at random. \n",
    "   Return:\n",
    "    1. A batch of images as a torch array with type torch.FloatTensor. \n",
    "    The first dimension of the array should be batch dimension, the second channel dimension, \n",
    "    followed by image height and image width. \n",
    "    2. Labels of the images in a torch array\n",
    "\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    batch_size = 4\n",
    "    # load train split\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./cifar10',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    \n",
    "    # take one rando mini-bathc\n",
    "    dataier = iter(trainloader)\n",
    "\n",
    "    images, labels = next(dataier)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "def train_classifier():\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    batch_size = 4\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./cifar10', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "    net = Net()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "    for epoch in range(2):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                running_loss = 0.0\n",
    "    print('Finished Training')\n",
    "    PATH = './cifar_net_2epoch.pth'\n",
    "    torch.save(net.state_dict(), PATH)\n",
    "    print(\"Saved trained weights to: \", PATH)\n",
    "\n",
    "def evalNetwork():\n",
    "    # Initialized the network and load from the saved weights\n",
    "    PATH = './cifar_net_2epoch.pth'\n",
    "    net = Net()\n",
    "    net.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "    # Loads dataset\n",
    "    batch_size=4\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    batch_size = 4\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./cifar10', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
    "\n",
    "def get_first_layer_weights():\n",
    "    net = Net()\n",
    "    # TODO: load the trained weights\n",
    "    PATH = './cifar_net_2epoch.pth'\n",
    "    net.load_state_dict(torch.load(PATH, weights_only=True))  # load saved weights\n",
    "    first_weight = net.conv1.weight.data.clone() # copy tensor\n",
    "    return first_weight\n",
    "\n",
    "def get_second_layer_weights():\n",
    "    net = Net()\n",
    "    # TODO: load the trained weights\n",
    "    PATH = './cifar_net_2epoch.pth'\n",
    "    net.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "    second_weight = net.conv2.weight.data.clone()# TODO: get conv2 weights (exclude bias)\n",
    "    return second_weight\n",
    "\n",
    "def hyperparameter_sweep():\n",
    "    '''\n",
    "    Reuse the CNN and training code from Question 2\n",
    "    Train the network three times using different learning rates: 0.01, 0.001, and 0.0001\n",
    "    During training, record the training loss every 2000 iterations\n",
    "    compute and record the training and test errors every 2000 iterations by randomly sampling 1000 images from each dataset\n",
    "    After training, plot three curves\n",
    "    '''\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    # train/test sets\n",
    "    trainset_full = torchvision.datasets.CIFAR10(\n",
    "        root='./cifar10',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    testset_full = torchvision.datasets.CIFAR10(\n",
    "        root='./cifar10',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    def make_loader(dataset, shuffle_flag):\n",
    "        batch_size = 4\n",
    "        return torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    \n",
    "    # helr to sample k random images and labels from a dataset.\n",
    "    # Returns:\n",
    "    #        images: Tensor [k, 3, 32, 32]\n",
    "    #        labels: LongTensor [k]\n",
    "    def sample_subset(dataset, k=1000):\n",
    "        idxs = random.sample(range(len(dataset)), k)\n",
    "        imgs_list = []\n",
    "        labs_list = []\n",
    "        for idx in idxs:\n",
    "            x, y = dataset[idx] \n",
    "            imgs_list.append(x.unsqueeze(0))  # [1,3,32,32]\n",
    "            labs_list.append(y)\n",
    "        imgs = torch.cat(imgs_list, dim=0)    # [k,3,32,32]\n",
    "        labs = torch.tensor(labs_list).long() # [k]\n",
    "        return imgs, labs\n",
    "    \n",
    "    # hlper to comupute classification error% on given tensors (images, labels)\n",
    "    # error% = 100*(1-accuarcy)\n",
    "    def eval_error_percent(net, imgs, labels):\n",
    "        with torch.no_grad():\n",
    "            outputs = net(imgs)                # [N,10]\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            total = labels.size(0)\n",
    "            err = 100.0 * (1.0 - (correct / total))\n",
    "        return err\n",
    "    # train new net for 2 pecos with a given learning rate: lr_value\n",
    "    # Returns: iters_log, loss_log, train_err_log, test_err_log\n",
    "    def train_once(lr_value):\n",
    "        net = Net()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr_value, momentum=0.9)\n",
    "\n",
    "        trainloader = make_loader(trainset_full, shuffle_flag=True)\n",
    "         # fixed 1k-train subset and 1k-test subset for this run\n",
    "        train_eval_imgs, train_eval_labs = sample_subset(trainset_full, k=1000)\n",
    "        test_eval_imgs, test_eval_labs = sample_subset(testset_full, k=1000)\n",
    "\n",
    "        iters_log = []\n",
    "        loss_log = []\n",
    "        train_err_log = []\n",
    "        test_err_log = []\n",
    "\n",
    "        running_loss = 0.0\n",
    "        iter_count = 0\n",
    "\n",
    "        for epoch in range(2):\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                inputs, labels = data\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                iter_count += 1\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # record stats every 2000 iters\n",
    "                if (iter_count % 2000) == 0:\n",
    "                    avg_loss = running_loss / 2000.0\n",
    "                    running_loss = 0.0\n",
    "\n",
    "                    # training error on 1k train subset\n",
    "                    train_err = eval_error_percent(net, train_eval_imgs, train_eval_labs)\n",
    "                    # test error on 1k test subset\n",
    "                    test_err = eval_error_percent(net, test_eval_imgs, test_eval_labs)\n",
    "\n",
    "                    iters_log.append(iter_count)\n",
    "                    loss_log.append(avg_loss)\n",
    "                    train_err_log.append(train_err)\n",
    "                    test_err_log.append(test_err)\n",
    "\n",
    "                    print(f\"[lr={lr_value} epoch {epoch+1} iter {iter_count}] \"\n",
    "                    f\"loss={avg_loss:.3f}, \"\n",
    "                    f\"train_err={train_err:.2f}%, \"\n",
    "                    f\"test_err={test_err:.2f}%\")\n",
    "\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cddd652f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[1,  2000] loss: 2.245\n",
      "[1,  4000] loss: 1.986\n",
      "[1,  6000] loss: 1.747\n",
      "[1,  8000] loss: 1.636\n",
      "[1, 10000] loss: 1.547\n",
      "[1, 12000] loss: 1.510\n",
      "[2,  2000] loss: 1.434\n",
      "[2,  4000] loss: 1.399\n",
      "[2,  6000] loss: 1.389\n",
      "[2,  8000] loss: 1.342\n",
      "[2, 10000] loss: 1.321\n",
      "[2, 12000] loss: 1.303\n",
      "Finished Training\n",
      "Saved trained weights to:  ./cifar_net_2epoch.pth\n",
      "Files already downloaded and verified\n",
      "Accuracy of the network on the 10000 test images: 52 %\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # your text code here\n",
    "\n",
    "    # sanity check: get batch tensors\n",
    "    #images, labels = CIFAR10_dataset_a()\n",
    "    #print(\"Batch images shape:\", images.shape)   # should be [4, 3, 32, 32]\n",
    "    #print(\"Batch labels:\", labels)               # should be 4 labels like tensor([..., ..., ..., ...])\n",
    "\n",
    "    # generate and show the visualization for the report\n",
    "    #isualize_four_images()\n",
    "    images, labels = CIFAR10_dataset_a()\n",
    "    train_classifier()\n",
    "    evalNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca2099b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eep596-cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
